{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f3a66b",
      "metadata": {
        "id": "c7f3a66b"
      },
      "outputs": [],
      "source": [
        "# === 1. Utilitarios mínimos (acurácia, dataset de exemplo, plot e padronização) ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return float((y_true == y_pred).mean())\n",
        "\n",
        "def make_moons(n=1000, noise=0.30, seed=0):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    angles = rng.rand(n//2) * np.pi\n",
        "    x1 = np.c_[np.cos(angles), np.sin(angles)]\n",
        "    x2 = np.c_[np.cos(angles), -np.sin(angles)] + [1.0, 0.4]\n",
        "    X = np.vstack([x1, x2])\n",
        "    y = np.r_[np.zeros(n//2, dtype=int), np.ones(n//2, dtype=int)]\n",
        "    X += rng.normal(scale=noise, size=X.shape)\n",
        "    return X, y\n",
        "\n",
        "def train_test_split(X, y, test_size=0.3, seed=0, stratify=True):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = X.shape[0]\n",
        "    if stratify:\n",
        "        tr, te = [], []\n",
        "        for c in np.unique(y):\n",
        "            ii = np.where(y == c)[0]\n",
        "            rng.shuffle(ii)\n",
        "            t = int(round((1 - test_size) * len(ii)))\n",
        "            tr.append(ii[:t]); te.append(ii[t:])\n",
        "        tr = np.concatenate(tr); te = np.concatenate(te)\n",
        "    else:\n",
        "        idx = np.arange(n); rng.shuffle(idx)\n",
        "        t = int(round((1 - test_size) * n))\n",
        "        tr, te = idx[:t], idx[t:]\n",
        "    return X[tr], X[te], y[tr], y[te]\n",
        "\n",
        "def fit_standardizer(X):\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    sd = X.std(axis=0, keepdims=True) + 1e-12\n",
        "    return mu, sd\n",
        "\n",
        "def transform_standardizer(X, mu, sd):\n",
        "    return (X - mu) / sd\n",
        "\n",
        "def plot_decision_boundary(model, X, y, h=0.03, proba=False, title=\"Fronteira de decisão\"):\n",
        "    x_min, x_max = X[:,0].min()-0.5, X[:,0].max()+0.5\n",
        "    y_min, y_max = X[:,1].min()-0.5, X[:,1].max()+0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    if proba and hasattr(model, \"predict_proba\"):\n",
        "        Z = model.predict_proba(grid)\n",
        "        if Z.ndim == 2 and Z.shape[1] > 1: Z = Z[:,1]\n",
        "    else:\n",
        "        Z = model.predict(grid)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
        "    #plt.scatter(X[:,0], X[:,1], c=y, edgecolor='k')\n",
        "    plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_lines(xs, ys_dict, xlabel=\"\", ylabel=\"\", title=\"\"):\n",
        "    plt.figure()\n",
        "    for label, ys in ys_dict.items():\n",
        "        plt.plot(xs, ys, marker=\"o\", label=label)\n",
        "    plt.xlabel(xlabel); plt.ylabel(ylabel); plt.title(title)\n",
        "    plt.legend(); plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882bdfd8",
      "metadata": {
        "id": "882bdfd8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 2. Regressão logística => OneHiddenMLP (ReLU + Softmax) ===\n",
        "import numpy as np\n",
        "\n",
        "# Rectified Linear Activation Unit\n",
        "def _relu(x): return np.maximum(0, x)\n",
        "def _relu_grad(x): return (x > 0).astype(float)\n",
        "\n",
        "def _softmax(z):\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    exp = np.exp(z)\n",
        "    return exp / (np.sum(exp, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "class OneHiddenMLP:\n",
        "    def __init__(self, n_hidden=64, lr=0.05, epochs=40, batch_size=64,\n",
        "                 l2=1e-4, random_state=0, verbose=False):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.l2 = l2\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.history_ = {}\n",
        "\n",
        "    def _init_params(self, n_in, n_out):\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "        W1 = rng.randn(n_in, self.n_hidden) * np.sqrt(2.0 / n_in)\n",
        "        b1 = np.zeros((1, self.n_hidden))\n",
        "        W2 = rng.randn(self.n_hidden, n_out) * np.sqrt(2.0 / self.n_hidden)\n",
        "        b2 = np.zeros((1, n_out))\n",
        "        self.params_ = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
        "\n",
        "    def _forward(self, X):\n",
        "        W1, b1, W2, b2 = self.params_[\"W1\"], self.params_[\"b1\"], self.params_[\"W2\"], self.params_[\"b2\"]\n",
        "        z1 = X @ W1 + b1\n",
        "        h1 = _relu(z1)\n",
        "        scores = h1 @ W2 + b2\n",
        "        probs = _softmax(scores)\n",
        "        return probs, {\"X\": X, \"z1\": z1, \"h1\": h1, \"scores\": scores, \"probs\": probs}\n",
        "\n",
        "    def _loss_and_grads(self, cache, y, n_classes):\n",
        "        X, z1, h1  = cache[\"X\"], cache[\"z1\"], cache[\"h1\"]\n",
        "        scores, probs= cache[\"scores\"], cache[\"probs\"]\n",
        "        m = X.shape[0]\n",
        "        Y = np.zeros((m, n_classes)); Y[np.arange(m), y.astype(int)] = 1.0\n",
        "        W1, W2 = self.params_[\"W1\"], self.params_[\"W2\"]\n",
        "\n",
        "        loss = -np.sum(Y * np.log(probs + 1e-12)) / m + 0.5*self.l2*(np.sum(W1*W1)+np.sum(W2*W2))\n",
        "        dscores = (probs - Y) / m\n",
        "        dW2 = h1.T @ dscores + self.l2 * W2\n",
        "        db2 = np.sum(dscores, axis=0, keepdims=True)\n",
        "        dh1 = dscores @ W2.T\n",
        "        dz1 = dh1 * _relu_grad(z1)\n",
        "        dW1 = X.T @ dz1 + self.l2 * W1\n",
        "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "        return loss, {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None, early_stopping=False, patience=10):\n",
        "        n_in = X.shape[1]; n_out = int(np.max(y)) + 1\n",
        "        self._init_params(n_in, n_out)\n",
        "        m = X.shape[0]\n",
        "\n",
        "        hist = {\"loss_tr\": [], \"acc_tr\": []}\n",
        "        if X_val is not None:\n",
        "            hist.update({\"loss_va\": [], \"acc_va\": []})\n",
        "\n",
        "        best_state = None; best_val = np.inf; wait = 0\n",
        "\n",
        "        for ep in range(self.epochs):\n",
        "            idx = np.arange(m); np.random.shuffle(idx)\n",
        "            for start in range(0, m, self.batch_size):\n",
        "                batch = idx[start:start+self.batch_size]\n",
        "                probs, cache = self._forward(X[batch])\n",
        "                loss, grads = self._loss_and_grads(cache, y[batch], n_out)\n",
        "                self.params_[\"W1\"] -= self.lr * grads[\"dW1\"]\n",
        "                self.params_[\"b1\"] -= self.lr * grads[\"db1\"]\n",
        "                self.params_[\"W2\"] -= self.lr * grads[\"dW2\"]\n",
        "                self.params_[\"b2\"] -= self.lr * grads[\"db2\"]\n",
        "\n",
        "            p_tr = self.predict_proba(X); yhat_tr = np.argmax(p_tr, axis=1)\n",
        "            loss_tr = -np.mean(np.log(p_tr[np.arange(m), y.astype(int)] + 1e-12))\n",
        "            hist[\"loss_tr\"].append(float(loss_tr))\n",
        "            hist[\"acc_tr\"].append(float((yhat_tr == y).mean()))\n",
        "\n",
        "            if X_val is not None:\n",
        "                pv = self.predict_proba(X_val); yhat_va = np.argmax(pv, axis=1)\n",
        "                loss_va = -np.mean(np.log(pv[np.arange(X_val.shape[0]), y_val.astype(int)] + 1e-12))\n",
        "                acc_va = float((yhat_va == y_val).mean())\n",
        "                hist[\"loss_va\"].append(float(loss_va))\n",
        "                hist[\"acc_va\"].append(acc_va)\n",
        "\n",
        "                if early_stopping:\n",
        "                    if loss_va < best_val - 1e-6:\n",
        "                        best_val = loss_va\n",
        "                        best_state = {k: v.copy() for k, v in self.params_.items()}\n",
        "                        wait = 0\n",
        "                    else:\n",
        "                        wait += 1\n",
        "                        if wait >= patience:\n",
        "                            if best_state is not None:\n",
        "                                self.params_ = best_state\n",
        "                            break\n",
        "\n",
        "        self.history_ = hist\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self._forward(X)[0]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return accuracy(y, self.predict(X))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3961c3c5",
      "metadata": {
        "id": "3961c3c5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3. Exemplo de treinamento do MLP em moons\n",
        "X, y = make_moons(n=1200, noise=0.30, seed=42)\n",
        "Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.35, seed=3, stratify=True)\n",
        "\n",
        "mu, sd = fit_standardizer(Xtr)\n",
        "Xtr_s = transform_standardizer(Xtr, mu, sd)\n",
        "Xva_s = transform_standardizer(Xva, mu, sd)\n",
        "\n",
        "mlp = OneHiddenMLP(n_hidden=64, lr=0.05, epochs=40, batch_size=64, l2=1e-4, random_state=0, verbose=False)\n",
        "mlp.fit(Xtr_s, ytr, X_val=Xva_s, y_val=yva, early_stopping=True, patience=8)\n",
        "\n",
        "print(\"Train acc:\", mlp.score(Xtr_s, ytr), \"| Val acc:\", mlp.score(Xva_s, yva))\n",
        "\n",
        "epochs = np.arange(1, len(mlp.history_[\"loss_tr\"])+1)\n",
        "ys = {\"loss_tr\": mlp.history_[\"loss_tr\"]}\n",
        "if \"loss_va\" in mlp.history_: ys[\"loss_va\"] = mlp.history_[\"loss_va\"]\n",
        "plot_lines(epochs, ys, xlabel=\"epoch\", ylabel=\"loss\", title=\"Loss vs epoch\")\n",
        "\n",
        "ys2 = {\"acc_tr\": mlp.history_[\"acc_tr\"]}\n",
        "if \"acc_va\" in mlp.history_: ys2[\"acc_va\"] = mlp.history_[\"acc_va\"]\n",
        "plot_lines(epochs, ys2, xlabel=\"epoch\", ylabel=\"accuracy\", title=\"Accuracy vs epoch\")\n",
        "\n",
        "plot_decision_boundary(mlp, Xtr_s, ytr, title=\"MLP (train space, standardized)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fa4bded",
      "metadata": {
        "id": "7fa4bded"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 4. Varredura de Hiperparametro (width & taxas de aprendizado) ===\n",
        "import matplotlib.pyplot as plt\n",
        "widths = [8, 16, 32, 64, 128]\n",
        "lrs = [0.01, 0.03, 0.05, 0.1]\n",
        "results = {}\n",
        "\n",
        "for w in widths:\n",
        "    vals = []\n",
        "    for lr in lrs:\n",
        "        m = OneHiddenMLP(n_hidden=w, lr=lr, epochs=30, batch_size=64, l2=1e-4, random_state=0)\n",
        "        m.fit(Xtr_s, ytr, X_val=Xva_s, y_val=yva, early_stopping=True, patience=5)\n",
        "        vals.append(m.score(Xva_s, yva))\n",
        "    results[f\"width={w}\"] = vals\n",
        "\n",
        "plt.figure()\n",
        "for label, vals in results.items():\n",
        "    plt.plot(lrs, vals, marker=\"o\", label=label)\n",
        "plt.xlabel(\"Taxa de aprendizado\"); plt.ylabel(\"Val accuracy\"); plt.title(\"Val acc vs LR para diferentes larguras\")\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbd4641",
      "metadata": {
        "id": "5fbd4641"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "np.random.seed(1)\n",
        "Xt = Xtr_s[:5]; yt = ytr[:5]\n",
        "m = OneHiddenMLP(n_hidden=5, lr=0.01, epochs=1, batch_size=5, l2=0.0, random_state=0)\n",
        "m._init_params(Xt.shape[1], int(np.max(y)+1))\n",
        "\n",
        "probs, cache = m._forward(Xt)\n",
        "loss, grads = m._loss_and_grads(cache, yt, int(np.max(y)+1))\n",
        "print(\"loss (analytical):\", loss)\n",
        "\n",
        "def loss_only(W2_new):\n",
        "    P, c = m._forward(Xt)\n",
        "    m.params_[\"W2\"] = W2_new\n",
        "    P, c = m._forward(Xt)\n",
        "    Y = np.zeros((Xt.shape[0], int(np.max(y)+1))); Y[np.arange(Xt.shape[0]), yt] = 1.0\n",
        "    return -np.sum(Y * np.log(P + 1e-12)) / Xt.shape[0]\n",
        "\n",
        "W2 = m.params_[\"W2\"].copy()\n",
        "num = np.zeros_like(W2)\n",
        "eps = 1e-5\n",
        "for i in range(W2.shape[0]):\n",
        "    for j in range(W2.shape[1]):\n",
        "        W2p = W2.copy(); W2p[i,j] += eps\n",
        "        W2m = W2.copy(); W2m[i,j] -= eps\n",
        "        m.params_[\"W2\"] = W2p; lp = loss_only(W2p)\n",
        "        m.params_[\"W2\"] = W2m; lm = loss_only(W2m)\n",
        "        num[i,j] = (lp - lm) / (2*eps)\n",
        "m.params_[\"W2\"] = W2\n",
        "\n",
        "rel_err = np.linalg.norm(num - grads[\"dW2\"]) / (np.linalg.norm(num) + np.linalg.norm(grads[\"dW2\"]) + 1e-12)\n",
        "print(\"Relative error (W2):\", rel_err)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}