{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aula 40 - Random Forest (02-10-2025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-da6dvApQOVt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Utilitarios mínimos (acurácia, dataset de exemplo e plot) ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return float((y_true == y_pred).mean())\n",
        "\n",
        "def confusion_matrix(y_true, y_pred, n_classes=None):\n",
        "    if n_classes is None:\n",
        "        n_classes = int(max(y_true.max(), y_pred.max())) + 1\n",
        "    M = np.zeros((n_classes, n_classes), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        M[int(t), int(p)] += 1\n",
        "    return M\n",
        "\n",
        "def make_moons(n=600, noise=0.25, seed=0):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    angles = rng.rand(n//2) * np.pi\n",
        "    x1 = np.c_[np.cos(angles), np.sin(angles)]\n",
        "    x2 = x1 + [1.0, -0.5]\n",
        "    X = np.vstack([x1, x2])\n",
        "    y = np.r_[np.zeros(n//2, dtype=int), np.ones(n//2, dtype=int)]\n",
        "    X += rng.normal(scale=noise, size=X.shape)\n",
        "    return X, y\n",
        "\n",
        "def plot_decision_boundary(model, X, y, h=0.03, proba=False, title=\"Fronteira de decisão\"):\n",
        "    x_min, x_max = X[:,0].min()-0.5, X[:,0].max()+0.5\n",
        "    y_min, y_max = X[:,1].min()-0.5, X[:,1].max()+0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    if proba and hasattr(model, \"predict_proba\"):\n",
        "        Z = model.predict_proba(grid)\n",
        "        if Z.ndim == 2 and Z.shape[1] > 1: Z = Z[:,1]\n",
        "    else:\n",
        "        Z = model.predict(grid)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, edgecolor='k')\n",
        "    plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "    plt.show()\n",
        "\n",
        "def train_test_split(X, y, test_size=0.3, seed=0, stratify=True):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = X.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    if stratify:\n",
        "        # estratificação estilo as que fizemos nos casos multiclasse anteriores\n",
        "        idx_tr = []\n",
        "        idx_te = []\n",
        "        for c in np.unique(y):\n",
        "            ii = np.where(y == c)[0]\n",
        "            rng.shuffle(ii)\n",
        "            t = int(round((1 - test_size) * len(ii)))\n",
        "            idx_tr.append(ii[:t]); idx_te.append(ii[t:])\n",
        "        idx_tr = np.concatenate(idx_tr); idx_te = np.concatenate(idx_te)\n",
        "    else:\n",
        "        rng.shuffle(idx)\n",
        "        t = int(round((1 - test_size) * n))\n",
        "        idx_tr, idx_te = idx[:t], idx[t:]\n",
        "    return X[idx_tr], X[idx_te], y[idx_tr], y[idx_te]\n",
        "\n",
        "def kfold_indices(n, k=5, shuffle=True, seed=0):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    idx = np.arange(n)\n",
        "    if shuffle:\n",
        "        rng.shuffle(idx)\n",
        "    folds = np.array_split(idx, k)\n",
        "    for i in range(k):\n",
        "        va = folds[i]\n",
        "        tr = np.concatenate([folds[j] for j in range(k) if j != i])\n",
        "        yield tr, va\n",
        "\n",
        "def plot_lines(xs, ys_dict, xlabel=\"\", ylabel=\"\", title=\"\"):\n",
        "    plt.figure()\n",
        "    for label, ys in ys_dict.items():\n",
        "        plt.plot(xs, ys, marker=\"o\", label=label)\n",
        "    plt.xlabel(xlabel); plt.ylabel(ylabel); plt.title(title)\n",
        "    plt.legend(); plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH8kly7MQWuw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def class_probs(y, n_classes=None):\n",
        "    y = y.astype(int)\n",
        "    if n_classes is None:\n",
        "        if y.size == 0:\n",
        "            return np.array([])\n",
        "        n_classes = int(np.max(y)) + 1\n",
        "    counts = np.bincount(y, minlength=n_classes).astype(float)\n",
        "    s = counts.sum()\n",
        "    return counts / s if s > 0 else counts\n",
        "\n",
        "def gini(y):\n",
        "    p = class_probs(y); return 1.0 - np.sum(p**2)\n",
        "\n",
        "def entropy(y, eps=1e-12):\n",
        "    p = class_probs(y); return -np.sum(p * np.log(p + eps))\n",
        "\n",
        "def impurity(y, criterion=\"gini\"):\n",
        "    return gini(y) if criterion == \"gini\" else entropy(y)\n",
        "\n",
        "def candidate_thresholds(x, max_candidates=50):\n",
        "    x = np.asarray(x); uniq = np.unique(x)\n",
        "    if uniq.size < 2: return np.array([])\n",
        "    if uniq.size > max_candidates:\n",
        "        qs = np.linspace(0, 1, max_candidates + 2)[1:-1]\n",
        "        thr = np.unique(np.quantile(x, qs))\n",
        "    else:\n",
        "        thr = (uniq[:-1] + uniq[1:]) / 2.0\n",
        "    return thr\n",
        "\n",
        "def split_on_threshold(X, y, j, thr):\n",
        "    xj = X[:, j]\n",
        "    L = xj <= thr\n",
        "    R = ~L\n",
        "    return L, R\n",
        "\n",
        "def weighted_impurity(yL, yR, criterion=\"gini\"):\n",
        "    m = yL.size + yR.size\n",
        "    if m == 0: return 0.0\n",
        "    return (yL.size * impurity(yL, criterion) + yR.size * impurity(yR, criterion)) / m\n",
        "\n",
        "def best_split(X, y, *, criterion=\"gini\", max_features=None, rng=None, max_candidates=50, stats=None):\n",
        "    m, n = X.shape\n",
        "    parent = impurity(y, criterion)\n",
        "    if m == 0 or parent <= 0.0:\n",
        "        return {\"gain\": 0.0, \"feat\": None, \"thr\": None}\n",
        "\n",
        "    feats = np.arange(n)\n",
        "    if max_features is not None:\n",
        "        if isinstance(max_features, str) and max_features == \"sqrt\":\n",
        "            k = max(1, int(np.sqrt(n)))\n",
        "        elif isinstance(max_features, int):\n",
        "            k = max(1, min(n, max_features))\n",
        "        else:\n",
        "            k = n\n",
        "        if rng is None: rng = np.random.RandomState(0)\n",
        "        feats = rng.choice(n, size=k, replace=False)\n",
        "\n",
        "    best = {\"gain\": 0.0, \"feat\": None, \"thr\": None}\n",
        "    cands = 0; valid = 0\n",
        "    for j in feats:\n",
        "        thrs = candidate_thresholds(X[:, j], max_candidates=max_candidates)\n",
        "        cands += thrs.size\n",
        "        for t in thrs:\n",
        "            L, R = split_on_threshold(X, y, j, t)\n",
        "            if not L.any() or not R.any():\n",
        "                continue\n",
        "            valid += 1\n",
        "            gain = parent - weighted_impurity(y[L], y[R], criterion)\n",
        "            if gain > best[\"gain\"]:\n",
        "                best = {\"gain\": float(gain), \"feat\": int(j), \"thr\": float(t)}\n",
        "    if stats is not None:\n",
        "        stats['cands'] = stats.get('cands', 0) + int(cands)\n",
        "        stats['valid'] = stats.get('valid', 0) + int(valid)\n",
        "    return best\n",
        "\n",
        "def _leaf(y, n_classes):\n",
        "    return {\"type\": \"leaf\", \"proba\": class_probs(y, n_classes=n_classes)}\n",
        "\n",
        "def build_tree(X, y, depth, *, max_depth=8, min_samples_split=2,\n",
        "               criterion=\"gini\", max_features=None, rng=None, max_candidates=50, stats=None, n_classes=None):\n",
        "    # Ensurse an effing fixed class dimension for the whole tree\n",
        "    if n_classes is None:\n",
        "        n_classes = int(np.max(y)) + 1 if y.size else 0\n",
        "\n",
        "    if (depth >= max_depth) or (X.shape[0] < min_samples_split) or (impurity(y, criterion) == 0.0):\n",
        "        return _leaf(y, n_classes)\n",
        "\n",
        "    split = best_split(X, y, criterion=criterion, max_features=max_features, rng=rng,\n",
        "                       max_candidates=max_candidates, stats=stats)\n",
        "    if split[\"feat\"] is None:\n",
        "        return _leaf(y, n_classes)\n",
        "\n",
        "    j, t = split[\"feat\"], split[\"thr\"]\n",
        "    L, R = split_on_threshold(X, y, j, t)\n",
        "    left = build_tree(X[L], y[L], depth+1, max_depth=max_depth, min_samples_split=min_samples_split,\n",
        "                      criterion=criterion, max_features=max_features, rng=rng,\n",
        "                      max_candidates=max_candidates, stats=stats, n_classes=n_classes)\n",
        "    right = build_tree(X[R], y[R], depth+1, max_depth=max_depth, min_samples_split=min_samples_split,\n",
        "                       criterion=criterion, max_features=max_features, rng=rng,\n",
        "                       max_candidates=max_candidates, stats=stats, n_classes=n_classes)\n",
        "    return {\"type\": \"node\", \"feat\": int(j), \"thr\": float(t), \"left\": left, \"right\": right, \"gain\": split[\"gain\"]}\n",
        "\n",
        "def predict_proba_row(node, x):\n",
        "    while node[\"type\"] != \"leaf\":\n",
        "        node = node[\"left\"] if x[node[\"feat\"]] <= node[\"thr\"] else node[\"right\"]\n",
        "    return node[\"proba\"]\n",
        "\n",
        "def predict_proba(tree, X):\n",
        "    return np.vstack([predict_proba_row(tree, x) for x in X])\n",
        "\n",
        "def predict(tree, X):\n",
        "    P = predict_proba(tree, X); return np.argmax(P, axis=1)\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=8, min_samples_split=2, criterion=\"gini\",\n",
        "                 max_features=None, random_state=0, max_candidates=50):\n",
        "        self.max_depth = max_depth; self.min_samples_split = min_samples_split\n",
        "        self.criterion = criterion; self.max_features = max_features\n",
        "        self.random_state = random_state; self.max_candidates = max_candidates\n",
        "        self.rng_ = np.random.RandomState(random_state); self.tree_ = None\n",
        "        self.n_classes_ = None\n",
        "        self.stats_ = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_classes_ = int(np.max(y)) + 1 if y.size else 0\n",
        "        self.stats_ = {}\n",
        "        self.tree_ = build_tree(\n",
        "            X, y, depth=0,\n",
        "            max_depth=self.max_depth, min_samples_split=self.min_samples_split,\n",
        "            criterion=self.criterion, max_features=self.max_features, rng=self.rng_,\n",
        "            max_candidates=self.max_candidates, stats=self.stats_, n_classes=self.n_classes_\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X): return predict_proba(self.tree_, X)\n",
        "    def predict(self, X): return np.argmax(self.predict_proba(X), axis=1)\n",
        "\n",
        "def trace_path(tree, x):\n",
        "    steps = []; node = tree\n",
        "    while node[\"type\"] != \"leaf\":\n",
        "        j, t = node[\"feat\"], node[\"thr\"]; go_left = x[j] <= t\n",
        "        steps.append((int(j), float(t), bool(go_left)))\n",
        "        node = node[\"left\"] if go_left else node[\"right\"]\n",
        "    return steps, node[\"proba\"]\n",
        "\n",
        "def pretty_print(node, depth=0):\n",
        "    import numpy as _np\n",
        "    indent = \"  \" * depth\n",
        "    if node[\"type\"] == \"leaf\":\n",
        "        print(f\"{indent}Leaf proba={_np.round(node['proba'], 3)}\")\n",
        "    else:\n",
        "        j, t = node[\"feat\"], node[\"thr\"]\n",
        "        print(f\"{indent}if x[{j}] <= {t:.4f}:\")\n",
        "        pretty_print(node[\"left\"], depth+1)\n",
        "        print(f\"{indent}else:  # x[{j}] > {t:.4f}\")\n",
        "        pretty_print(node[\"right\"], depth+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDBxgdu6QeSD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "class BaggingClassifier:\n",
        "    def __init__(self, base_estimator, n_estimators=100, bootstrap=True, random_state=0):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.bootstrap = bootstrap\n",
        "        self.random_state = random_state\n",
        "        self.estimators_ = []\n",
        "        self.n_classes_ = None\n",
        "        self.oob_score_ = None\n",
        "        self.rng_ = np.random.RandomState(random_state)\n",
        "\n",
        "    def _pad_proba(self, P):\n",
        "        # ensures shape (n_samples, n_classes_)\n",
        "        if P.shape[1] == self.n_classes_:\n",
        "            return P\n",
        "        out = np.zeros((P.shape[0], self.n_classes_), dtype=float)\n",
        "        out[:, :P.shape[1]] = P\n",
        "        return out\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n = X.shape[0]\n",
        "        self.n_classes_ = int(np.max(y)) + 1\n",
        "        self.estimators_ = []\n",
        "        oob_votes = [ [] for _ in range(n) ]\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            est = deepcopy(self.base_estimator)\n",
        "            if self.bootstrap:\n",
        "                idx = self.rng_.randint(0, n, size=n)\n",
        "            else:\n",
        "                idx = np.arange(n)\n",
        "            est.fit(X[idx], y[idx])\n",
        "            self.estimators_.append(est)\n",
        "\n",
        "            if self.bootstrap:\n",
        "                mask = np.ones(n, dtype=bool); mask[idx] = False\n",
        "                if mask.any():\n",
        "                    P = est.predict_proba(X[mask])\n",
        "                    P = self._pad_proba(P)\n",
        "                    for j, p in zip(np.where(mask)[0], P):\n",
        "                        oob_votes[j].append(p)\n",
        "\n",
        "        has = np.array([len(v) > 0 for v in oob_votes])\n",
        "        if has.any():\n",
        "            agg = np.zeros((has.sum(), self.n_classes_), dtype=float)\n",
        "            ii = np.where(has)[0]\n",
        "            for k, j in enumerate(ii):\n",
        "                agg[k] = np.mean(oob_votes[j], axis=0)\n",
        "            preds = np.argmax(agg, axis=1)\n",
        "            self.oob_score_ = float((preds == y[has]).mean())\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        Ps = []\n",
        "        for est in self.estimators_:\n",
        "            P = est.predict_proba(X)\n",
        "            P = self._pad_proba(P)\n",
        "            Ps.append(P)\n",
        "        return np.mean(Ps, axis=0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)\n",
        "\n",
        "class RandomForest:\n",
        "    def __init__(self, n_estimators=100, max_depth=8, min_samples_split=2, max_features=\"sqrt\", random_state=0):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.bagger_ = None\n",
        "        self.oob_score_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        base = DecisionTree(max_depth=self.max_depth,\n",
        "                            min_samples_split=self.min_samples_split,\n",
        "                            criterion=\"gini\",\n",
        "                            max_features=self.max_features,\n",
        "                            random_state=self.random_state)\n",
        "        self.bagger_ = BaggingClassifier(base_estimator=base,\n",
        "                                         n_estimators=self.n_estimators,\n",
        "                                         bootstrap=True,\n",
        "                                         random_state=self.random_state).fit(X, y)\n",
        "        self.oob_score_ = self.bagger_.oob_score_\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X): return self.bagger_.predict_proba(X)\n",
        "    def predict(self, X): return self.bagger_.predict(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN6JCUBAQjZN"
      },
      "outputs": [],
      "source": [
        "# Célula de exemplos\n",
        "\n",
        "X, y = make_moons(n=1200, noise=0.30, seed=7)\n",
        "Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.35, seed=3, stratify=True)\n",
        "\n",
        "# árvore base (pré-poda)\n",
        "baseline = DecisionTree(max_depth=6, criterion=\"gini\", random_state=0).fit(Xtr, ytr)\n",
        "base_va = accuracy(yva, baseline.predict(Xva))\n",
        "print(\"Single tree (max_depth=6) — Val acc:\", round(base_va, 4))\n",
        "\n",
        "grid = [1, 5, 10, 20, 40, 80, 160]\n",
        "bag_va, bag_oob = [], []\n",
        "rf_va,  rf_oob  = [], []\n",
        "\n",
        "for n_est in grid:\n",
        "    # Bagging (no subsampling)\n",
        "    bag = BaggingClassifier(\n",
        "        base_estimator=DecisionTree(max_depth=6, criterion=\"gini\", random_state=0),\n",
        "        n_estimators=n_est, bootstrap=True, random_state=0\n",
        "    ).fit(Xtr, ytr)\n",
        "    bag_va.append(accuracy(yva, bag.predict(Xva)))\n",
        "    bag_oob.append(bag.oob_score_)\n",
        "\n",
        "    # Random Forest (sqrt feature subsampling)\n",
        "    rf = RandomForest(n_estimators=n_est, max_depth=6, max_features=\"sqrt\", random_state=0).fit(Xtr, ytr)\n",
        "    rf_va.append(accuracy(yva, rf.predict(Xva)))\n",
        "    rf_oob.append(rf.oob_score_)\n",
        "\n",
        "plot_lines(grid, {\n",
        "    \"Bagging (Val)\": bag_va,\n",
        "    \"RandomForest (Val)\": rf_va,\n",
        "    \"Single tree (Val, baseline)\": [base_va]*len(grid),\n",
        "}, xlabel=\"# Árvores\", ylabel=\"Acurácia\", title=\"Val accuracy vs # Árvores\")\n",
        "\n",
        "plot_lines(grid, {\n",
        "    \"Bagging (OOB)\": bag_oob,\n",
        "    \"RandomForest (OOB)\": rf_oob,\n",
        "}, xlabel=\"# Árvores\", ylabel=\"OOB accuracy\", title=\"OOB vs # Árvores (sem conjunto de validação)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfzJd_sHfpO5"
      },
      "outputs": [],
      "source": [
        "plot_decision_boundary(baseline, X, y, title=\"Árvore única (max_depth=6)\")\n",
        "rf_best = RandomForest(n_estimators=160, max_depth=6, max_features=\"sqrt\", random_state=0).fit(Xtr, ytr)\n",
        "plot_decision_boundary(rf_best, X, y, title=\"Random Forest (160 árvores, max_depth=6)\")\n",
        "print(\"RF — Val acc:\", accuracy(yva, rf_best.predict(Xva)), \"| OOB:\", rf_best.oob_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBrDrESfSRPk"
      },
      "outputs": [],
      "source": [
        "plot_decision_boundary(rf_best, X, y, proba=True, title=\"Random Forest (160 árvores, max_depth=6)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBkZRpDv7yA5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Função recursiva para varrer a árvore e coletar os ganhos\n",
        "def get_tree_importances(node, importances_arr):\n",
        "    if node[\"type\"] == \"node\":\n",
        "        # Soma o ganho desse split para a importancia desse feature\n",
        "        importances_arr[node[\"feat\"]] += node[\"gain\"]\n",
        "        # Recursa para os ramos esquerdo e direto\n",
        "        get_tree_importances(node[\"left\"], importances_arr)\n",
        "        get_tree_importances(node[\"right\"], importances_arr)\n",
        "\n",
        "# Conta principal\n",
        "# Pega o número de features do dataset\n",
        "n_features = Xtr.shape[1]\n",
        "\n",
        "# Array que guarda todas as importâncias, incializado em zero\n",
        "total_importances = np.zeros(n_features)\n",
        "\n",
        "# Varre todas as árvores da floresta\n",
        "for tree in rf_best.bagger_.estimators_:\n",
        "    # importâncias dessa árvore\n",
        "    single_tree_importances = np.zeros(n_features)\n",
        "    # Chama a função que calcula as importâncias nessa árvore\n",
        "    get_tree_importances(tree.tree_, single_tree_importances)\n",
        "    # Soma a importância dessa árvore\n",
        "    total_importances += single_tree_importances\n",
        "\n",
        "# média entre todas as árvores\n",
        "total_importances /= len(rf_best.bagger_.estimators_)\n",
        "\n",
        "print(\"Importâncias:\", total_importances)\n",
        "\n",
        "\n",
        "feature_names = [f\"Feature {i}\" for i in range(n_features)]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_names, total_importances)\n",
        "plt.title(\"Importância de Features\")\n",
        "plt.ylabel(\"Importância\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mE_ZkBcQkpd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiyhRx4OQu2p"
      },
      "source": [
        "# Exercícios\n",
        "\n",
        "1. Na célula de exemplos, no Bagging Classifier, modifique bootstrap=True para bootstrap=False\n",
        "- O que acontece o valor das acurácias de validação?\n",
        "- Por que isso acontece? Como são as árvores em que o ensemble é treinado?\n",
        "\n",
        "2. Na célula de exemplos, no classificador RandomForest, troque max_features=\"sqrt\" por max_features=None.\n",
        "- Como fica a acurácia de validação de RandomForest?\n",
        "- Qual a diferença conceitual entre esse modelo de random forest que você rodou dessa forma e o BaggingClassifier?\n",
        "\n",
        "3. Triplique a célula de exemplos, e em um delas troque todas a max_depths por 2. Em outra, troque todas a max_depth por 20.\n",
        "- O que acontece quando as árvores base são extremamente rasas?\n",
        "- Quando as árvores são profundas, o RandomForest consegue mitigar os problemas de overfitting?\n",
        "\n",
        "4. Crie uma função capaz de calcular a importância de uma feature a partir da análise da perda de acurácia quando você embaralha uma feature."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
