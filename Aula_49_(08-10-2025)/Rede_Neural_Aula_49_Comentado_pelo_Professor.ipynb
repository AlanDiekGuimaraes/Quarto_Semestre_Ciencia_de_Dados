{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A porta lógica AND (E) só retorna 1 (Verdadeiro) se TODAS as entradas forem 1. Isso exige que o Limiar seja mais alto do que na porta OR.\n",
        "\n"
      ],
      "metadata": {
        "id": "5H8Z60V_TVpK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlX8KdrmTTzY"
      },
      "outputs": [],
      "source": [
        "# EXERCÍCIO 2: SIMULANDO UM NEURÔNIO ARTIFICIAL (PORTA LÓGICA AND)\n",
        "\n",
        "# Esta função simula o neurônio configurado para a lógica AND.\n",
        "def neuronio_simples_and(entrada_x1, entrada_x2):\n",
        "    \"\"\"\n",
        "    Simula um único neurônio (Perceptron) para a porta lógica AND.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Definir Pesos e Limiar\n",
        "    # Os pesos (W1 e W2) continuam 1.\n",
        "    w1 = 1\n",
        "    w2 = 1\n",
        "    # O Limiar (T = 1.5) é o novo \"ajuste fino\". É a chave para a lógica AND.\n",
        "    # Ele garante que apenas (1 AND 1) irá gerar o disparo.\n",
        "    limiar = 1.5\n",
        "\n",
        "    print(f\"-> Entradas: X1={entrada_x1}, X2={entrada_x2}\")\n",
        "\n",
        "    # 2. Cálculo da Soma Ponderada\n",
        "    # O cálculo é o mesmo: (X1 * W1) + (X2 * W2).\n",
        "    soma_ponderada = (entrada_x1 * w1) + (entrada_x2 * w2)\n",
        "\n",
        "    print(f\"-> Soma Ponderada: {soma_ponderada}\")\n",
        "\n",
        "    # 3. Função de Ativação\n",
        "    # A lógica de decisão também é a mesma: verifica se a soma ultrapassou o limiar.\n",
        "    if soma_ponderada > limiar:\n",
        "        # Saída 1: Dispara apenas se a soma for 2.0 (1 e 1)\n",
        "        saida = 1\n",
        "    else:\n",
        "        # Saída 0: Não Dispara se a soma for 0.0, 1.0 ou 1.0.\n",
        "        saida = 0\n",
        "\n",
        "    print(f\"-> Limiar (T): {limiar}. Saída: {saida}\\n\")\n",
        "    return saida\n",
        "\n",
        "# --- Testando o Neurônio AND ---\n",
        "# Título: Apenas o último caso deve retornar 1.\n",
        "print(\"--- Testes AND (Saída Esperada: 0, 0, 0, 1) ---\")\n",
        "neuronio_simples_and(0, 0) # 0 AND 0 -> 0 (Soma=0.0. Não Dispara)\n",
        "neuronio_simples_and(0, 1) # 0 AND 1 -> 0 (Soma=1.0. Não Dispara)\n",
        "neuronio_simples_and(1, 0) # 1 AND 0 -> 0 (Soma=1.0. Não Dispara)\n",
        "neuronio_simples_and(1, 1) # 1 AND 1 -> 1 (Soma=2.0. Dispara)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A porta lógica NOT (NÃO) inverte a entrada: se a entrada for 1, a saída é 0, e vice-versa. Para isso, precisamos de um peso negativo. O peso negativo é a forma do neurônio de implementar uma conexão inibitória."
      ],
      "metadata": {
        "id": "InbSeKPgTqJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCÍCIO 3: SIMULANDO UM NEURÔNIO ARTIFICIAL (PORTA LÓGICA NOT)\n",
        "\n",
        "# Esta função simula o neurônio configurado para a lógica NOT.\n",
        "# Ela recebe apenas uma entrada.\n",
        "def neuronio_simples_not(entrada_x1):\n",
        "    \"\"\"\n",
        "    Simula um único neurônio (Perceptron) para a porta lógica NOT.\n",
        "    Demonstra o uso de pesos negativos (conexões inibitórias).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Definir Peso e Limiar\n",
        "    # O peso W1 (-2) é negativo para INIBIR o disparo quando a entrada for 1.\n",
        "    w1 = -2\n",
        "    # O Limiar (-1) é baixo (negativo) para garantir o disparo quando a soma é 0 (Entrada X1=0).\n",
        "    limiar = -1\n",
        "\n",
        "    print(f\"-> Entrada: X1={entrada_x1}\")\n",
        "\n",
        "    # 2. Cálculo da Soma Ponderada\n",
        "    # O cálculo considera apenas a única entrada.\n",
        "    soma_ponderada = (entrada_x1 * w1)\n",
        "\n",
        "    print(f\"-> Soma Ponderada: {soma_ponderada}\")\n",
        "\n",
        "    # 3. Função de Ativação\n",
        "    # A decisão é a mesma: comparar a soma com o limiar.\n",
        "    if soma_ponderada > limiar:\n",
        "        # Saída 1: Acontece quando X1=0 (Soma=0.0), pois 0.0 > -1.0.\n",
        "        saida = 1\n",
        "    else:\n",
        "        # Saída 0: Acontece quando X1=1 (Soma=-2.0), pois -2.0 não é > -1.0.\n",
        "        saida = 0\n",
        "\n",
        "    print(f\"-> Limiar (T): {limiar}. Saída: {saida}\\n\")\n",
        "    return saida\n",
        "\n",
        "# --- Testando o Neurônio NOT ---\n",
        "# Título: A saída deve ser o inverso da entrada.\n",
        "print(\"--- Testes NOT (Saída Esperada: 1, 0) ---\")\n",
        "neuronio_simples_not(0) # NOT 0 -> 1\n",
        "neuronio_simples_not(1) # NOT 1 -> 0"
      ],
      "metadata": {
        "id": "G-MVZ2feTqoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vamos simular uma rede que resolve a porta lógica XOR (OU Exclusivo). A porta XOR é famosa porque é o problema mais simples que um único neurônio não consegue resolver, exigindo que se use, no mínimo, uma rede com uma camada oculta.\n",
        "\n",
        "O desafio da porta XOR (OU Exclusivo) exige que a rede use a seguinte estratégia:\n",
        "\n",
        "Neurônio 1 (OR): Garante que o sinal passe se qualquer entrada for 1.\n",
        "\n",
        "Neurônio 2 (NAND): Garante que o sinal não passe se ambas as entradas forem 1 (o caso que deve ser 0).\n",
        "\n",
        "Neurônio 3 (AND): O neurônio final só dispara se receber o sinal do OR (dizendo que pelo menos um é 1) E do NAND (dizendo que as entradas não são iguais a 1 e 1).\n",
        "\n",
        "A combinação dessas lógicas resolve o problema!\n"
      ],
      "metadata": {
        "id": "eKWvIdcUUQHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# PARTE 1: FUNÇÃO DE ATIVAÇÃO BÁSICA\n",
        "# Esta função é compartilhada por TODOS os neurônios da rede.\n",
        "# ====================================================================\n",
        "\n",
        "def funcao_ativacao_degrau(soma_ponderada, limiar_T):\n",
        "    \"\"\"\n",
        "    Implementa a Função de Ativação (Step Function).\n",
        "    Se a soma ultrapassa o limiar, o neurônio dispara (saída = 1).\n",
        "    \"\"\"\n",
        "    # Verifica se o sinal interno do neurônio é forte o suficiente.\n",
        "    if soma_ponderada > limiar_T:\n",
        "        return 1\n",
        "    # Caso contrário, o neurônio permanece inativo.\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# ====================================================================\n",
        "# PARTE 2: NEURÔNIOS DA CAMADA OCULTA (Recebem X1 e X2)\n",
        "# Cada função representa um neurônio com seus pesos e limiares fixos.\n",
        "# ====================================================================\n",
        "\n",
        "def neuronio_or(x1, x2):\n",
        "    \"\"\"\n",
        "    Neurônio 1: Configurado para a lógica OR (OU).\n",
        "    Dispara se *pelo menos uma* entrada for 1.\n",
        "    \"\"\"\n",
        "    # Pesos (W1=1, W2=1) e Limiar (T=0.5) ajustados para a lógica OR.\n",
        "    w1, w2, limiar = 1, 1, 0.5\n",
        "\n",
        "    # Cálculo da Soma Ponderada: (X1*W1) + (X2*W2)\n",
        "    soma = (x1 * w1) + (x2 * w2)\n",
        "\n",
        "    # Retorna o resultado da ativação (0 ou 1).\n",
        "    return funcao_ativacao_degrau(soma, limiar)\n",
        "\n",
        "def neuronio_nand(x1, x2):\n",
        "    \"\"\"\n",
        "    Neurônio 2: Configurado para a lógica NAND (NOT AND).\n",
        "    Dispara se *não* for (1 E 1). Usamos pesos negativos para isso.\n",
        "    \"\"\"\n",
        "    # Pesos (W1=-1, W2=-1) e Limiar (T=-1.5) ajustados para a lógica NAND.\n",
        "    w1, w2, limiar = -1, -1, -1.5\n",
        "\n",
        "    # Cálculo da Soma Ponderada: (X1*W1) + (X2*W2)\n",
        "    soma = (x1 * w1) + (x2 * w2)\n",
        "\n",
        "    # Retorna o resultado da ativação (0 ou 1).\n",
        "    return funcao_ativacao_degrau(soma, limiar)\n",
        "\n",
        "# ====================================================================\n",
        "# PARTE 3: NEURÔNIO DA CAMADA DE SAÍDA (Recebe as saídas N1 e N2)\n",
        "# ====================================================================\n",
        "\n",
        "def neuronio_and(saida_n1, saida_n2):\n",
        "    \"\"\"\n",
        "    Neurônio 3: Configurado para a lógica AND (E).\n",
        "    Dispara se receber 1 *de ambos* os neurônios da camada oculta.\n",
        "    \"\"\"\n",
        "    # Pesos (W1=1, W2=1) e Limiar (T=1.5) ajustados para a lógica AND.\n",
        "    w1, w2, limiar = 1, 1, 1.5\n",
        "\n",
        "    # Cálculo da Soma Ponderada: (Saída N1 * W1) + (Saída N2 * W2)\n",
        "    soma = (saida_n1 * w1) + (saida_n2 * w2)\n",
        "\n",
        "    # Retorna o resultado final da rede (0 ou 1).\n",
        "    return funcao_ativacao_degrau(soma, limiar)\n",
        "\n",
        "# ====================================================================\n",
        "# PARTE 4: A REDE NEURAL (ORQUESTRADOR)\n",
        "# ====================================================================\n",
        "\n",
        "def rede_neural_xor(x1, x2):\n",
        "    \"\"\"\n",
        "    Função principal que coordena o fluxo de dados através das camadas.\n",
        "    \"\"\"\n",
        "    print(f\"--- Processando entradas: X1={x1}, X2={x2} ---\")\n",
        "\n",
        "    # FLUXO 1: CAMADA OCULTA\n",
        "\n",
        "    # 1.1. O sinal (X1, X2) entra no Neurônio OR.\n",
        "    saida_n1 = neuronio_or(x1, x2)\n",
        "    print(f\"  > Saída N1 (OR): {saida_n1}\")\n",
        "\n",
        "    # 1.2. O mesmo sinal (X1, X2) entra no Neurônio NAND.\n",
        "    saida_n2 = neuronio_nand(x1, x2)\n",
        "    print(f\"  > Saída N2 (NAND): {saida_n2}\")\n",
        "\n",
        "    # FLUXO 2: CAMADA DE SAÍDA\n",
        "\n",
        "    # 2.1. As saídas da camada oculta se tornam as entradas para o Neurônio AND (N3).\n",
        "    entradas_para_saida = [saida_n1, saida_n2]\n",
        "    print(f\"  > Entradas do N3: {entradas_para_saida}\")\n",
        "\n",
        "    # 2.2. N3 calcula o resultado final.\n",
        "    saida_final = neuronio_and(saida_n1, saida_n2)\n",
        "\n",
        "    print(f\"  > Saída Final (XOR): {saida_final}\\n\")\n",
        "\n",
        "    return saida_final\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# TESTES\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "print(\"--- Testando a Porta Lógica XOR (Saída Esperada: 0, 1, 1, 0) ---\")\n",
        "\n",
        "# 0 XOR 0 -> Esperado: 0\n",
        "rede_neural_xor(0, 0)\n",
        "\n",
        "# 0 XOR 1 -> Esperado: 1\n",
        "rede_neural_xor(0, 1)\n",
        "\n",
        "# 1 XOR 0 -> Esperado: 1\n",
        "rede_neural_xor(1, 0)\n",
        "\n",
        "# 1 XOR 1 -> Esperado: 0\n",
        "rede_neural_xor(1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aP7BCzAVTzl",
        "outputId": "06bc81c4-d47f-48d2-bf00-a3df47ad7ec3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testando a Porta Lógica XOR (Saída Esperada: 0, 1, 1, 0) ---\n",
            "--- Processando entradas: X1=0, X2=0 ---\n",
            "  > Saída N1 (OR): 0\n",
            "  > Saída N2 (NAND): 1\n",
            "  > Entradas do N3: [0, 1]\n",
            "  > Saída Final (XOR): 0\n",
            "\n",
            "--- Processando entradas: X1=0, X2=1 ---\n",
            "  > Saída N1 (OR): 1\n",
            "  > Saída N2 (NAND): 1\n",
            "  > Entradas do N3: [1, 1]\n",
            "  > Saída Final (XOR): 1\n",
            "\n",
            "--- Processando entradas: X1=1, X2=0 ---\n",
            "  > Saída N1 (OR): 1\n",
            "  > Saída N2 (NAND): 1\n",
            "  > Entradas do N3: [1, 1]\n",
            "  > Saída Final (XOR): 1\n",
            "\n",
            "--- Processando entradas: X1=1, X2=1 ---\n",
            "  > Saída N1 (OR): 1\n",
            "  > Saída N2 (NAND): 0\n",
            "  > Entradas do N3: [1, 0]\n",
            "  > Saída Final (XOR): 0\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}